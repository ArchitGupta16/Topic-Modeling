{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchitGupta16/Topic-Modeling/blob/main/Pipeline_Anvil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCApKdLJJcno"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install hdbscan\n",
        "!pip install umap-learn\n",
        "!pip install anvil-uplink\n",
        "\n",
        "import anvil.server\n",
        "import anvil.media\n",
        "import matplotlib.pyplot as plt\n",
        "import umap.umap_ as umap\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap.umap_ as um\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "anvil.server.connect(\"server_UVYNGR7VHA6XPK4JVKXPQYIC-43YI7Q4JG4YWXLGL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbEdn4fJbXnl"
      },
      "outputs": [],
      "source": [
        "import chardet\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHOAtqJ5Cru8"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def file_reader(file):\n",
        "  with open('filename', 'wb') as f:\n",
        "        f.write(file.get_bytes())\n",
        "\n",
        "    # Create a Media object from the uploaded file\n",
        "  media_object = anvil.media.from_file('filename')\n",
        "\n",
        "  # Read the contents of the media object\n",
        "  data = media_object.get_bytes()\n",
        "  print(data)\n",
        "  # encoding = chardet.detect(data)['encoding']\n",
        "  data_text = data.decode()\n",
        "  print(data_text)\n",
        "  return data_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O23kLT-5l2KZ",
        "outputId": "1e20aa27-6591-4296-be14-306e488b4933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/Summary/umap_model_summary.pkl', 'rb') as f:\n",
        "      umap_embeddings = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/Summary/dbscan_model_summary.pkl', 'rb') as f:\n",
        "    cluster = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgFtbbJpmCPw"
      },
      "outputs": [],
      "source": [
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "      count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
        "      t = count.transform(documents).toarray()\n",
        "      w = t.sum(axis=1)\n",
        "      tf = np.divide(t.T, w)\n",
        "      sum_t = t.sum(axis=0)\n",
        "      idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "      tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "      return tf_idf, count\n",
        "\n",
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "      words = count.get_feature_names_out()\n",
        "      labels = list(docs_per_topic.Topic)\n",
        "      tf_idf_transposed = tf_idf.T\n",
        "      indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "      top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "      return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "      topic_sizes = (df.groupby(['Topic']).Doc.count().reset_index().rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns').sort_values(\"Size\", ascending=False))\n",
        "      return topic_sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRT18L6Hlmeu"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def topic_modelling(inp):\n",
        "\n",
        "  l = inp.split(\"@@@@@\")\n",
        "  # for line in inp:\n",
        "  #   l.append(line)\n",
        "  # print(len(l))\n",
        "  print(l)\n",
        "  print(type(l),len(l))\n",
        "  model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "  embeddings = model.encode(l, show_progress_bar=True)\n",
        "  # print(embeddings)\n",
        "\n",
        "  new_embeddings= umap_embeddings.transform(embeddings)\n",
        "\n",
        "  labels_test = cluster.fit_predict(new_embeddings)\n",
        "\n",
        "\n",
        "  umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "  result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
        "  result['labels'] = cluster.labels_\n",
        "\n",
        "  # Visualize clusters\n",
        "  fig, ax = plt.subplots(figsize=(20, 10))\n",
        "  outliers = result.loc[result.labels == -1, :]\n",
        "  clustered = result.loc[result.labels != -1, :]\n",
        "  plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
        "  plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
        "\n",
        "  #@title c TF-IDF\n",
        "\n",
        "  docs_df = pd.DataFrame(l, columns=[\"Doc\"])\n",
        "  docs_df['Topic'] = cluster.labels_\n",
        "  docs_df['Doc_ID'] = range(len(docs_df))\n",
        "  docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "\n",
        "\n",
        "  tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(l))\n",
        "  top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "  topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n",
        "\n",
        "  print(type(top_n_words[-1][:10]),top_n_words[-1][:10])\n",
        "\n",
        "  doc_pos = {}\n",
        "\n",
        "\n",
        "  for i in tqdm(range(len(l))):\n",
        "    document = nlp(l[i])\n",
        "    # Token and Tag\n",
        "    temp = {}\n",
        "\n",
        "    for token in document:\n",
        "      a = str(token)\n",
        "      if token.pos_==\"PROPN\":\n",
        "        temp[a] = \"PROPN\"\n",
        "\n",
        "\n",
        "      elif token.pos_==\"NOUN\":\n",
        "        temp[a] = \"NOUN\"\n",
        "\n",
        "      elif token.pos_==\"VERB\":\n",
        "        temp[a] = \"VERB\"\n",
        "\n",
        "      else:\n",
        "        temp[a] = \"OTHER\"\n",
        "\n",
        "      doc_pos[i]=temp\n",
        "\n",
        "  tem=[]\n",
        "  for i in doc_pos.items():\n",
        "\n",
        "    for j,k in i[1].items():\n",
        "\n",
        "      if k == 'NOUN' or k == 'PROPN':\n",
        "        tem.append(j)\n",
        "    #if i[1]=='NOUN':\n",
        "      #print(i[0])\n",
        "  # print(tem)\n",
        "\n",
        "  topics = {}\n",
        "  t = []\n",
        "\n",
        "  for i in range(-1,len(top_n_words)-1):\n",
        "    for j in range(0,20):\n",
        "      if top_n_words[i][j][0] in tem:\n",
        "        t.append(top_n_words[i][j])\n",
        "    topics[i] = t\n",
        "    t = []\n",
        "\n",
        "  output = \"\"\n",
        "  for i in topics[-1]:\n",
        "    print(i[0])\n",
        "    output += \" \" + i[0]\n",
        "  print(output)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mXOhrAgbmLtV"
      },
      "outputs": [],
      "source": [
        "anvil.server.wait_forever()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}