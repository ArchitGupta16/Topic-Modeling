{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchitGupta16/Topic-Modeling/blob/main/Pipeline_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVa_dg6iB_B4"
      },
      "outputs": [],
      "source": [
        "#@title Data And Packakges\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# cats = ['sci.space']\n",
        "data = fetch_20newsgroups(subset='all')['data']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[29])"
      ],
      "metadata": {
        "id": "My-DRd15jLqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fp3-1YSHZ23l",
        "outputId": "8b355a6d-08e9-49d7-a0da-ac338d5a8b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18846"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddDDlbxbsJX2",
        "outputId": "01a404cd-c702-4daa-e907-4ee3dadc6e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhbXncBOrRvt"
      },
      "outputs": [],
      "source": [
        "#@title Summarization\n",
        "model_name = \"t5-base\"\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsLi3nLEa1Uh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxAV_x5jqTdv"
      },
      "outputs": [],
      "source": [
        "final= [None]*len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW3Z7aHYayqL"
      },
      "outputs": [],
      "source": [
        "copy = [None]*len(data)\n",
        "for i in range(0,len(data)):\n",
        "  copy[i] = data[i].split(\",From\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQXGMMLXrT4l"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(len(copy))):\n",
        "  tex=\"\"\n",
        "  if type(copy[i]) == list:\n",
        "    tex = ''.join([str(elem) for elem in copy[i]])\n",
        "  else:\n",
        "    tex = copy[i]\n",
        "  input_ids = tokenizer.encode(\"summarize: \" + tex, return_tensors=\"pt\", add_special_tokens=True)\n",
        "  generated_ids = model.generate(input_ids=input_ids,num_beams=5,max_length=100,repetition_penalty=2.5,length_penalty=1,early_stopping=True,num_return_sequences=1)\n",
        "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "  final[i]=preds\n",
        "  tex = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CS4kyR-AGb6"
      },
      "outputs": [],
      "source": [
        "flat_list = []\n",
        "for sublist in final:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwKBiWeirWLN",
        "outputId": "7feb8078-7273-4339-ddeb-8edd095a57f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Drive - for uploading summary\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW4gJGKHulS9"
      },
      "outputs": [],
      "source": [
        "writeBook = open(\"/content/gdrive/MyDrive/Colab Notebooks/R&D/data.txt\", \"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPFDwYenrjIU"
      },
      "outputs": [],
      "source": [
        "# cont = open(\"{}.txt\".format('data'), \"a\")\n",
        "for i in range(len(flat_list)):\n",
        "  writeBook.write(flat_list[i]+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOEQgIFq26E-"
      },
      "outputs": [],
      "source": [
        "writeBook.close()\n",
        "# writeBook = open(\"{}.txt\".format('data'), \"w+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i4hY3DBGXQV"
      },
      "outputs": [],
      "source": [
        "#@title Summary Content from Drive\n",
        "inp = open(\"/content/gdrive/MyDrive/Colab Notebooks/R&D/0_11000.txt\", \"r\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikjTxnaasK2h",
        "outputId": "8dcaf392-35bb-4bfe-b538-855e77f3e5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/gdrive/MyDrive/Colab Notebooks/R&D/0_11000.txt' mode='r' encoding='UTF-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEVhbevAHtj9"
      },
      "outputs": [],
      "source": [
        "l = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hInsTMQGdgh",
        "outputId": "daccc5a7-089e-43c7-aa9f-c7c715c3b5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "for line in inp.readlines():\n",
        "  l.append(line)\n",
        "l = l[:1000]\n",
        "l\n",
        "print(len(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YRJG1z4__n"
      },
      "outputs": [],
      "source": [
        "data = copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrTIYZkEDOW-"
      },
      "outputs": [],
      "source": [
        "#@title Library\n",
        "# !python -m pip install Pytorch\n",
        "# !python -m pip install SentenceTransformer\n",
        "!pip install -U sentence-transformers\n",
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VluWhdWCSqT"
      },
      "outputs": [],
      "source": [
        "#@title Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "embeddings = model.encode(l, show_progress_bar=True)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmbDgJEK2wuq"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gGZCM5XIl-Z"
      },
      "outputs": [],
      "source": [
        "#@title UMAP\n",
        "import umap.umap_ as um\n",
        "umap_embeddings = um.UMAP(n_neighbors=15, n_components=2, metric='cosine').fit_transform(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6dzWRKBMqW5"
      },
      "outputs": [],
      "source": [
        "#@title HDBSCAN\n",
        "import hdbscan\n",
        "cluster = hdbscan.HDBSCAN(min_samples=5,min_cluster_size=75,metric='euclidean', cluster_selection_method='leaf').fit(umap_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUs58wK2Mz4a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import umap.umap_ as umap\n",
        "import pandas as pd\n",
        "# Prepare data\n",
        "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
        "result['labels'] = cluster.labels_\n",
        "\n",
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5bu0c5nM3PT"
      },
      "outputs": [],
      "source": [
        "#@title c TF-IDF\n",
        "docs_df = pd.DataFrame(l, columns=[\"Doc\"])\n",
        "docs_df['Topic'] = cluster.labels_\n",
        "docs_df['Doc_ID'] = range(len(docs_df))\n",
        "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q01ZUgjM-mF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "\n",
        "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImgH2i9pNBXj"
      },
      "outputs": [],
      "source": [
        "#@title Topic Representation\n",
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names_out()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['Topic']).Doc.count().reset_index().rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns').sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hISKqs0T1Mov"
      },
      "outputs": [],
      "source": [
        "top_n_words[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE-F09CHNGrZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(20):\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(tf_idf.T)\n",
        "    np.fill_diagonal(similarities, 0)\n",
        "\n",
        "    # Extract label to merge into and from where\n",
        "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
        "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
        "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
        "\n",
        "    # Adjust topics\n",
        "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
        "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
        "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
        "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
        "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "\n",
        "    # Calculate new topic words\n",
        "    m = len(l)\n",
        "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
        "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "\n",
        "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKVEs2PBvFAM",
        "outputId": "b598d751-c16b-478f-c5c7-9a090e186369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('year', 0.03776856683976911),\n",
              " ('clutch', 0.02979047171254766),\n",
              " ('days', 0.027647128635489175),\n",
              " ('90', 0.026833018928390996),\n",
              " ('asbestos', 0.025758058809400554),\n",
              " ('car', 0.025547778936764954),\n",
              " ('homicides', 0.024744498953434207),\n",
              " ('handgun', 0.022772863763996433),\n",
              " ('left', 0.021127821375493384),\n",
              " ('10', 0.01944761259441615)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_n_words[4][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71ZQGGTcBJC0"
      },
      "outputs": [],
      "source": [
        "len(top_n_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-5vljPD9DTr"
      },
      "outputs": [],
      "source": [
        "#@title Pre-processing Coherence\n",
        "tops = []\n",
        "l = []\n",
        "for i in top_n_words.values():\n",
        "  for j in i:\n",
        "    l.append(j[0])\n",
        "  tops.append(l)\n",
        "  l = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqP0XfNvnRue"
      },
      "outputs": [],
      "source": [
        "#@title Coherence\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK-qEKOMnSXB"
      },
      "outputs": [],
      "source": [
        "texts, _ = fetch_20newsgroups( subset='all',return_X_y=True )\n",
        "tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
        "texts = [ tokenizer(t) for t in  texts ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euMqXIrEnUm8"
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary with the vocabulary\n",
        "word2id = Dictionary( texts )\n",
        "\n",
        "# Coherence model\n",
        "cm = CoherenceModel(topics=tops, texts=texts, coherence='c_v', dictionary=word2id)\n",
        "coherence_per_topic = cm.get_coherence_per_topic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aArB_gr5C1MD"
      },
      "outputs": [],
      "source": [
        "coherence_per_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i52M0IBVDCts"
      },
      "outputs": [],
      "source": [
        "sum(coherence_per_topic)/len(coherence_per_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgVJ0DLenNNs"
      },
      "outputs": [],
      "source": [
        "#@title Pre-processing Perplexity\n",
        "l = []\n",
        "perplex = []\n",
        "count = 0\n",
        "for i in top_n_words.values():\n",
        "  for j in i:\n",
        "    l.append(j[0])\n",
        "  perplex.append(l)\n",
        "  count+=1\n",
        "  l = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J1mFevWnb6J"
      },
      "outputs": [],
      "source": [
        "#@title Perplexity\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYAn20zfWuZT",
        "outputId": "96f09611-dd1e-48c5-e0cd-b535dd66f3a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "import collections, nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ZJEZeYWwXp"
      },
      "outputs": [],
      "source": [
        "corpus, _ = fetch_20newsgroups( subset='all',return_X_y=True )\n",
        "print(type(corpus))\n",
        "tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
        "tokens = [ tokenizer(t) for t in  corpus ]\n",
        "\n",
        "flat = []\n",
        "for element in tokens:\n",
        "    if type(element) is list:\n",
        "        for item in element:\n",
        "            flat.append(item)\n",
        "    else:\n",
        "        flat.append(element)\n",
        "\n",
        "tokens = flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rasuyYYUWyfc"
      },
      "outputs": [],
      "source": [
        "def unigram(tokens):\n",
        "    model = collections.defaultdict(lambda: 0.01)\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model[f] += 1\n",
        "        except KeyError:\n",
        "            model [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model.values()))\n",
        "    for word in model:\n",
        "        model[word] = model[word]/N\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQstaB3lW0tR"
      },
      "outputs": [],
      "source": [
        "#computes perplexity of the unigram model on a testset\n",
        "def perplexity(testset, model):\n",
        "    testset = testset.split()\n",
        "    perplexity = 1\n",
        "    N = 0\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        perplexity = perplexity * (1/model[word])\n",
        "    perplexity = pow(perplexity, 1/float(N))\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "model = unigram(tokens)\n",
        "dic = {}\n",
        "# for i in perplex:\n",
        "  # for j in i:\n",
        "  #   dic[j] = perplexity(j,model)\n",
        "count=0\n",
        "for i in perplex:\n",
        "  d = {}\n",
        "  count += 1\n",
        "  for j in i:\n",
        "    d[j] = None\n",
        "  dic[count] = d\n",
        "print(dic)\n",
        "\n",
        "\n",
        "for i in dic.values():\n",
        "  print(i)\n",
        "  for key, value in i.items():\n",
        "    print(key,value)\n",
        "    i[key] = perplexity(key,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpLcXmPS0Fnn"
      },
      "outputs": [],
      "source": [
        "dic"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}